## Open Problems in Reinforcement Learning

I met Harm Van Seijen at Microsoft Bangalore few weeks back. He has done his PhD under Richard Sutton. It was really interesting to talk to him and know about some of the open research problems in this domain. Here are few points that I remember:

1.  Mostly his talk focused on introducing topics  like Monte Carlo, Temporal Differencing, SARSA, Q-Learning etc (Which you taught in the class same slides mostly)
2.  Second half was new where he talked mostly about the open problems in RL like:
      a.    Efficient exploration techniques: Currently papers like DQN uses simple exploration techniques like epsilon greedy/ softmax exploration. These are inefficient and becomes obsolete once reward signals becomes sparse. So, there is lot of scope in this track.
     b.    Safety Approach : This  is mostly important in areas where we need real time performance like robot movement, obstacle avoidance by drone where experiments are expensive and time consuming to fix and restart again. This requires developing methods to incorporate safety in a variety of areas like  exploration and  modifying reward definition.

3. According to me RL can boost performance in many AI based applications. So, one of my idea was to solve stock price prediction problem using RL but recently Sriraj Raval famous youtuber posted about the same on twitter


